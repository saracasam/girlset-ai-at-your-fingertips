# Girlset — AI at Your Fingertips

A short, hands-on crash course on **Responsible AI** created for Girlset (Concordia).
## Contents
- `slides/` — workshop slide deck
- `activities/` and facilitation notes
  
## Workshop goals
- Demystify where AI shows up in everyday life
- Introduce key risk areas (healthcare bias, hiring algorithms, facial recognition)
- Facilitate reflection on **who builds AI** and why that matters
- End with a quick activity inspired by *Affecting Machines* to humanize algorithm design
  
## How to run this workshop (60–75 min)
- **Audience:** high-school / undergrad beginners
- **Group size:** 10–40
- **Room:** projector + speakers + sticky notes
- **Materials:** printed “If I built AI…” slips (Slide 28), trading cards (Slide 21, optional)
- **Flow:**  
  1) Icebreaker & “Can you trust AI?” (Slides 1–5, 10 min)  
  2) Everyday AI + GAN demo (Slides 6–16, 10–12 min)  
  3) Case studies & small reflections (Slides 17–20, 15–20 min)  
  4) Targeted ads + design-a-fix (Slides 24–25, 10–12 min)  
  5) Airport screening fairness (Slides 26–27, 8–10 min)  
  6) Personal pledges (Slides 28–29, 5–8 min)

## Slides
- 👉 [Download the deck](./slides/GirlSetAIDeck.pdf)

## References
- **Facial recognition & wrongful arrest:** ACLU case page + settlement (Robert Williams).  
  - https://www.aclu.org/cases/williams-v-city-of-detroit-face-recognition-false-arrest  
  - Settlement docs + updates.  
- **Hiring algorithm bias:** Reuters on Amazon’s resume screener (2018).  
  - https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/  
- **Healthcare triage bias:** Obermeyer et al., *Science* (2019) + explainer.  
  - https://www.science.org/doi/10.1126/science.aax2342  
  - https://pubmed.ncbi.nlm.nih.gov/31649194/

## License
- **Original Girlset materials** © 2025 Sara Jameel — licensed under **CC BY 4.0**.  
- **Adapted content from third parties** is used with permission and may carry different terms. 

